{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4faa8829-384f-4d9d-b3a9-ba3c2fafda5f",
   "metadata": {},
   "source": [
    "2. Improve computer vision accuracy with convolutions\n",
    "You now know how to do fashion image recognition using a Deep Neural Network (DNN) containing three layers— the input layer (in the shape of the input data), the output layer (in the shape of the desired output) and a hidden layer. You experimented with several parameters that influence the final accuracy, such as different sizes of hidden layers and number of training epochs.\n",
    "\n",
    "For convenience, here's the entire code again. Run it and take a note of the test accuracy that is printed out at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f6940479-f97a-47b7-8795-effbae89da2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-29 17:14:40.798929: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-29 17:14:40.802642: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-29 17:14:40.822673: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-01-29 17:14:40.860130: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1738170880.909572  572948 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1738170880.924863  572948 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-29 17:14:41.012557: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-29 17:14:48.040321: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7827 - loss: 0.6183\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.8600 - loss: 0.3877 \n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.8766 - loss: 0.3369\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.8881 - loss: 0.3123\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.8936 - loss: 0.2911\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.8773 - loss: 0.3359\n",
      "Test loss: 0.3435840904712677, Test accuracy: 87.51999735832214\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images/255.0\n",
    "test_images=test_images/255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1697c6-6f50-48ed-a050-3f5acd8ea8a3",
   "metadata": {},
   "source": [
    "Your accuracy is probably about 89% on training and 87% on validation. You can make that even better using convolutions, which narrows down the content of the image to focus on specific, distinct details.\n",
    "\n",
    "If you've ever done image processing using a filter, then convolutions will look very familiar.\n",
    "\n",
    "In short, you take an array (usually 3x3 or 5x5) and pass it over the image. By changing the underlying pixels based on the formula within that matrix, you can perform operations like edge detection. For example, typically a 3x3 is defined for edge detection where the middle cell is 8, and all of its neighbors are -1. In this case, for each pixel, you would multiply its value by 8, then subtract the value of each neighbor. Do this for every pixel, and you'll end up with a new image that has its edges enhanced.\n",
    "\n",
    "This is perfect for computer vision, because enhancing features like edges helps the computer distinguish one item from another. Better still, the amount of information needed is much less, because you'll train only on the highlighted features.\n",
    "\n",
    "That's the concept of Convolutional Neural Networks. Add some layers to do convolution before you have the dense layers, and then the information going to the dense layers becomes more focused and possibly more accurate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b04cf18-d35e-4ad8-ba13-aa70f59a2913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">36,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m640\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m36,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">243,786</span> (952.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m243,786\u001b[0m (952.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">243,786</span> (952.29 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m243,786\u001b[0m (952.29 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 27ms/step - accuracy: 0.7770 - loss: 0.6142\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 26ms/step - accuracy: 0.8886 - loss: 0.3037\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 26ms/step - accuracy: 0.9082 - loss: 0.2506\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 26ms/step - accuracy: 0.9180 - loss: 0.2186\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 27ms/step - accuracy: 0.9297 - loss: 0.1868 \n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 13ms/step - accuracy: 0.8994 - loss: 0.2809\n",
      "Test loss: 0.27336207032203674, Test accuracy: 90.14999866485596\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images=training_images / 255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images=test_images / 255.0\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(2,2),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64509145-5b6f-4a6d-8b93-a525298f1029",
   "metadata": {},
   "source": [
    "Lets try adding more epos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e6fe248-943b-44d2-9c12-0dbfec32894a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 27ms/step - accuracy: 0.9370 - loss: 0.1669\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 26ms/step - accuracy: 0.9438 - loss: 0.1467\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 24ms/step - accuracy: 0.9532 - loss: 0.1240\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 25ms/step - accuracy: 0.9592 - loss: 0.1081\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 25ms/step - accuracy: 0.9630 - loss: 0.0949\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.9059 - loss: 0.3319\n",
      "Test loss: 0.3157525360584259, Test accuracy: 90.77000021934509\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e8859a5-8677-43df-abd5-f861cc9093b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 28ms/step - accuracy: 0.9672 - loss: 0.0853\n",
      "Epoch 2/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 24ms/step - accuracy: 0.9709 - loss: 0.0756\n",
      "Epoch 3/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 23ms/step - accuracy: 0.9772 - loss: 0.0632\n",
      "Epoch 4/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 22ms/step - accuracy: 0.9771 - loss: 0.0605\n",
      "Epoch 5/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 23ms/step - accuracy: 0.9817 - loss: 0.0511\n",
      "Epoch 6/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 23ms/step - accuracy: 0.9827 - loss: 0.0469\n",
      "Epoch 7/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 23ms/step - accuracy: 0.9839 - loss: 0.0423\n",
      "Epoch 8/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 22ms/step - accuracy: 0.9842 - loss: 0.0409\n",
      "Epoch 9/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 23ms/step - accuracy: 0.9881 - loss: 0.0351\n",
      "Epoch 10/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 22ms/step - accuracy: 0.9873 - loss: 0.0350\n",
      "Epoch 11/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 21ms/step - accuracy: 0.9898 - loss: 0.0293\n",
      "Epoch 12/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 25ms/step - accuracy: 0.9883 - loss: 0.0312\n",
      "Epoch 13/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 23ms/step - accuracy: 0.9893 - loss: 0.0288\n",
      "Epoch 14/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 22ms/step - accuracy: 0.9895 - loss: 0.0288\n",
      "Epoch 15/15\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 24ms/step - accuracy: 0.9907 - loss: 0.0270 \n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 11ms/step - accuracy: 0.9047 - loss: 0.6386\n",
      "Test loss: 0.6155976057052612, Test accuracy: 91.0099983215332\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model.fit(training_images, training_labels, epochs=15)\n",
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6442f5e-ac2a-482a-a9f2-6310ad2453f3",
   "metadata": {},
   "source": [
    "While the training results might seem really good, the validation results may actually go down due to a phenomenon called overfitting.\n",
    "\n",
    "Overfitting occurs when the network learns the data from the training set too well, so it's specialised to recognize only that data, and as a result is less effective at seeing other data in more general situations. For example, if you trained only on heels, then the network might be very good at identifying heels, but sneakers might confuse it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "239cb8c2-f693-4f89-8866-7b2b647094ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.fashion_mnist\n",
    "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
    "training_images=training_images.reshape(60000, 28, 28, 1)\n",
    "training_images = training_images/255.0\n",
    "test_images = test_images.reshape(10000, 28, 28, 1)\n",
    "test_images = test_images/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ed927-c9b5-41f1-bd52-133c2b5e04ba",
   "metadata": {},
   "source": [
    "4. Gather the data\n",
    "The first step is to gather the data.\n",
    "\n",
    "You'll notice that there's a change here and the training data needed to be reshaped. That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, you have a single 4D list that is 60,000x28x28x1, and the same for the test images. If you don't do that, then you'll get an error when training because the convolutions do not recognize the shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d5d27f5-c12a-438a-ac77-73c1035a3c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
    "tf.keras.layers.MaxPooling2D(2, 2),\n",
    "#Add another convolution\n",
    "tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
    "tf.keras.layers.MaxPooling2D(2, 2),\n",
    "#Now flatten the output. After this you'll just have the same DNN structure as the non convolutional version\n",
    "tf.keras.layers.Flatten(),\n",
    "#The same 128 dense layers, and 10 output layers as in the pre-convolution example:\n",
    "tf.keras.layers.Dense(128, activation='relu'),\n",
    "tf.keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f29161c-724c-42d4-a0b2-2205f6fe35a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 17ms/step - accuracy: 0.7870 - loss: 0.5945\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 16ms/step - accuracy: 0.8907 - loss: 0.2992\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 18ms/step - accuracy: 0.9074 - loss: 0.2502\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 16ms/step - accuracy: 0.9222 - loss: 0.2095\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 17ms/step - accuracy: 0.9322 - loss: 0.1831\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 8ms/step - accuracy: 0.9046 - loss: 0.2588\n",
      "Test loss: 0.2519456446170807, Test accuracy: 90.67999720573425\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "481e4df7-e03e-4e69-a626-cf87f561c459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 2 1 1 6 1 4 6 5 7 4 5 7 3 4 1 2 4 8 0 2 5 7 9 1 4 6 0 9 3 8 8 3 3 8 0 7\n",
      " 5 7 9 6 1 3 7 6 7 2 1 2 2 4 4 5 8 2 2 8 4 8 0 7 7 8 5 1 1 2 3 9 8 7 0 2 6\n",
      " 2 3 1 2 8 4 1 8 5 9 5 0 3 2 0 6 5 3 6 7 1 8 0 1 4 2]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0ea22f3-0a95-46cc-b5f8-80fea385c820",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "The layer sequential_2 has never been called and thus has no defined input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m      8\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m [layer\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers]\n\u001b[0;32m----> 9\u001b[0m activation_model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mModel(inputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m, outputs \u001b[38;5;241m=\u001b[39m layer_outputs)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m     11\u001b[0m   f1 \u001b[38;5;241m=\u001b[39m activation_model\u001b[38;5;241m.\u001b[39mpredict(test_images[FIRST_IMAGE]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m28\u001b[39m, \u001b[38;5;241m1\u001b[39m))[x]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/keras/src/ops/operation.py:254\u001b[0m, in \u001b[0;36mOperation.input\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minput\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Retrieves the input tensor(s) of a symbolic operation.\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \n\u001b[1;32m    248\u001b[0m \u001b[38;5;124;03m    Only returns the tensor(s) corresponding to the *first time*\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;124;03m        Input tensor or list of input tensors.\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_node_attribute_at_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_tensors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/keras/src/ops/operation.py:285\u001b[0m, in \u001b[0;36mOperation._get_node_attribute_at_index\u001b[0;34m(self, node_index, attr, attr_name)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Private utility to retrieves an attribute (e.g. inputs) from a node.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03mThis is used to implement the properties:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m    The operation's attribute `attr` at the node of index `node_index`.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes:\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has never been called \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand thus has no defined \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    288\u001b[0m     )\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes) \u001b[38;5;241m>\u001b[39m node_index:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    291\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to get \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattr_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m at node \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but the operation has only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    293\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inbound_nodes)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m inbound nodes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    294\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: The layer sequential_2 has never been called and thus has no defined input."
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "f, axarr = plt.subplots(3,4)\n",
    "FIRST_IMAGE=0\n",
    "SECOND_IMAGE=23\n",
    "THIRD_IMAGE=28\n",
    "CONVOLUTION_NUMBER = 6\n",
    "from tensorflow.keras import models\n",
    "layer_outputs = [layer.output for layer in model.layers]\n",
    "activation_model = tf.keras.models.Model(inputs = model.input, outputs = layer_outputs)\n",
    "for x in range(0,4):\n",
    "  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[0,x].grid(False)\n",
    "  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[1,x].grid(False)\n",
    "  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n",
    "  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n",
    "  axarr[2,x].grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67d247a7-41a8-4c98-8a5d-305ad18987e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39fb5362-b9af-48f4-af9e-9239d424abc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(32,), output.shape=(32, 26, 26, 32)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(test_images, test_labels)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest loss: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, Test accuracy: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(test_loss, test_acc\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/keras/src/backend/tensorflow/nn.py:725\u001b[0m, in \u001b[0;36msparse_categorical_crossentropy\u001b[0;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    720\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must be at least rank 1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    721\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    722\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    723\u001b[0m     )\n\u001b[1;32m    724\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[0;32m--> 725\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    726\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument `output` must have rank (ndim) `target.ndim - 1`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    728\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, output.shape=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    729\u001b[0m     )\n\u001b[1;32m    730\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e1, e2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target\u001b[38;5;241m.\u001b[39mshape, output\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]):\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m e1 \u001b[38;5;241m!=\u001b[39m e2:\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `output` must have rank (ndim) `target.ndim - 1`. Received: target.shape=(32,), output.shape=(32, 26, 26, 32)"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(training_images, training_labels, epochs=5)\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print ('Test loss: {}, Test accuracy: {}'.format(test_loss, test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9625b2c-5a79-44b8-8712-0a9c6b9e02b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
